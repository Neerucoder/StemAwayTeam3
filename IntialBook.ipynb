{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IntialBook.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wp3C2dcXN7g1"
      },
      "source": [
        "Ressource : [https://rruntsch.medium.com/how-to-write-a-python-program-to-query-biomedical-journal-citations-in-the-pubmed-database-c7e842e4df89](https://rruntsch.medium.com/how-to-write-a-python-program-to-query-biomedical-journal-citations-in-the-pubmed-database-c7e842e4df89) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SzQiTXpiZ-l"
      },
      "source": [
        "## Oumaima"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zq-MLxJMgly"
      },
      "source": [
        "import json # PubMed database searches retrieve results in JSON format. This module provides functions to access data in JSON structures.\n",
        "import urllib.request # The urlopen() function sends the request URL to the NCBI server to execute and retrieve search results and citations.\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P76JRDxJM5JX"
      },
      "source": [
        "def retrieve_abstract():       \n",
        "    urls = []\n",
        "    base_url_efetch = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?\"\n",
        "    output_file_path = r\"/pubmed_xml_fies/\"\n",
        "    sleep_minutes = .2\n",
        "        \n",
        "    ids = ['13n'+str(n).zfill(4) for n in range(1,3)]\n",
        "    for id in ids:\n",
        "        url = base_url_efetch + 'db=pubmed&id=' + id + '&retmode=text&retmax=text&rettype=abstract'\n",
        "        urls.append(url)\n",
        "        result = urllib.request.urlopen(url)\n",
        "        text = result.read().decode('utf-8')\n",
        "        file_name = output_file_path + \"pubmed_\" + id + \".txt\"\n",
        "        \n",
        "        file_out = open(file_name, \"w\", encoding=\"utf-8\")\n",
        "        file_out.write(text)\n",
        "        file_out.close()\n",
        "        \n",
        "        # Sleep so that pubmed is not overloaded\n",
        "        time.sleep(sleep_minutes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVFC3OTXMZHn"
      },
      "source": [
        "## Tanish"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIswgzDyN4BZ"
      },
      "source": [
        "retrieve_abstract()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGSKp-rVirNW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ac48612-f12b-4ba9-b711-19b1d5fdd0ec"
      },
      "source": [
        "!pip install git+git://github.com/titipata/pubmed_parser.git #This needs to be run in order for the pubmed_parser to work"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/titipata/pubmed_parser.git\n",
            "  Cloning git://github.com/titipata/pubmed_parser.git to /tmp/pip-req-build-h2cd5vfi\n",
            "  Running command git clone -q git://github.com/titipata/pubmed_parser.git /tmp/pip-req-build-h2cd5vfi\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from pubmed-parser==0.2.2) (4.2.6)\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pubmed-parser==0.2.2) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pubmed-parser==0.2.2) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pubmed-parser==0.2.2) (1.19.5)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from pubmed-parser==0.2.2) (3.6.4)\n",
            "Collecting pytest-cov\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/84/576b071aef9ac9301e5c0ff35d117e12db50b87da6f12e745e9c5f745cc2/pytest_cov-2.12.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pubmed-parser==0.2.2) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pubmed-parser==0.2.2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pubmed-parser==0.2.2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pubmed-parser==0.2.2) (2021.5.30)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->pubmed-parser==0.2.2) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->pubmed-parser==0.2.2) (1.10.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->pubmed-parser==0.2.2) (1.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->pubmed-parser==0.2.2) (57.0.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->pubmed-parser==0.2.2) (21.2.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->pubmed-parser==0.2.2) (8.8.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from pytest-cov->pubmed-parser==0.2.2) (0.10.2)\n",
            "Collecting coverage>=5.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/e0/fc9f7bd9b84e6b41d0aad1a113e36714aac0c0a9b307aca5f9af443bc50f/coverage-5.5-cp37-cp37m-manylinux2010_x86_64.whl (242kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 13.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pubmed-parser\n",
            "  Building wheel for pubmed-parser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pubmed-parser: filename=pubmed_parser-0.2.2-cp37-none-any.whl size=18334 sha256=9692a29dfe8cd95416293c53d7f6127d0a29cc1b7c4d6f48b856bd1778a95982\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8uudkwm1/wheels/f5/4c/84/40073501015d7126a260f0cdae4ebae8a37e8e480ee36ff9ce\n",
            "Successfully built pubmed-parser\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement coverage==3.7.1, but you'll have coverage 5.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: coveralls 0.5 has requirement coverage<3.999,>=3.6, but you'll have coverage 5.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pytest-cov 2.12.1 has requirement pytest>=4.6, but you'll have pytest 3.6.4 which is incompatible.\u001b[0m\n",
            "Installing collected packages: unidecode, coverage, pytest-cov, pubmed-parser\n",
            "  Found existing installation: coverage 3.7.1\n",
            "    Uninstalling coverage-3.7.1:\n",
            "      Successfully uninstalled coverage-3.7.1\n",
            "Successfully installed coverage-5.5 pubmed-parser-0.2.2 pytest-cov-2.12.1 unidecode-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG_hwBWZigzB"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "import pubmed_parser as pp\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYKEQU9pi2HG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f281b83-92d1-41f5-b6d8-ad8d7b3ef60b"
      },
      "source": [
        "url_list = []\n",
        "numbers_in_file_name = range(1, 3) #1062 is max so use 1063 if attempting to access the last file. CAUTION: pulling too many .xml files probably will cause this to crash. \n",
        "total_dataframe = pd.DataFrame() #Empty dataframe which will eventually contain all data parsed using the PubMed parser\n",
        "\n",
        "for n in numbers_in_file_name:\n",
        "  url_list.append(\"https://ftp.ncbi.nlm.nih.gov/pubmed/baseline/pubmed21n\" + str(n).zfill(4) + \".xml.gz\") #Creating relevant links to pull data. \n",
        "\n",
        "for n in url_list:\n",
        "  print(n) #Printing those links to double-check. \n",
        "\n",
        "for url in url_list:\n",
        "  r = requests.get(url, stream = True) \n",
        "\n",
        "  with open(url[45:62],\"wb\") as xml:\n",
        "    for chunk in r.iter_content(chunk_size=1024):\n",
        "        if chunk:\n",
        "            xml.write(chunk) #Pulling Data and writing it to the working directory (which in this case is \"\\content\"). \n",
        "  raw_data = pp.parse_medline_xml(\"/content/\" + url[45:62]) #Parsing data\n",
        "  #print(raw_data[1:3]) #Can be used to check raw_data\n",
        "  initial_list = list(raw_data) #Making sure the data is a list and putting values into a new variable. \n",
        "  new_df = pd.DataFrame(initial_list) #Creating dataframe with the batch of parsed data.\n",
        "  total_dataframe = total_dataframe.append(new_df, ignore_index = True) #Appending the dataframe created above to the total dataframe containing all parsed data. \n",
        "    \n",
        "\n",
        "print(\"\\n\", total_dataframe.head(100), \"\\n\") #Double-checking to make sure the data came out right. \n",
        "\n",
        "\n",
        "pruned_dataframe = total_dataframe[['pmid','pubdate','abstract']] #Contains only the relevant columns that we need for the Stanford Parser. \n",
        "pruned_dataframe['pmid'] = pruned_dataframe['pmid'].astype(int) #Ignore the error concerning the SettingWithCopyWarning. I can't seem to get around it and the data type for the two columns does seem to change anyways. \n",
        "pruned_dataframe['pubdate'] = pruned_dataframe['pubdate'].astype(int) #\n",
        "\n",
        "print(pruned_dataframe.head(100), \"\\n\") #Double-checking data\n",
        "print(pruned_dataframe.dtypes, \"\\n\") #Double-checking column data types "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://ftp.ncbi.nlm.nih.gov/pubmed/baseline/pubmed21n0001.xml.gz\n",
            "https://ftp.ncbi.nlm.nih.gov/pubmed/baseline/pubmed21n0002.xml.gz\n",
            "\n",
            "                                                 title  ...              country\n",
            "0   Formate assay in body fluids: application in m...  ...        United States\n",
            "1   Delineation of the intimate details of the bac...  ...        United States\n",
            "2   Metal substitutions incarbonic anhydrase: a ha...  ...        United States\n",
            "3   Effect of chloroquine on cultured fibroblasts:...  ...        United States\n",
            "4   Atomic models for the polypeptide backbones of...  ...        United States\n",
            "..                                                ...  ...                  ...\n",
            "95  Hydrogen peroxide and iron: a microbial cellul...  ...        United States\n",
            "96  [Luminescence study of the effect of temperatu...  ...  Russia (Federation)\n",
            "97                    Microbial sources of cellulase.  ...        United States\n",
            "98   Mannosidosis: clinical and biochemical findings.  ...        United States\n",
            "99  Bovine mannosidosis--a model lysosomal storage...  ...        United States\n",
            "\n",
            "[100 rows x 20 columns] \n",
            "\n",
            "    pmid  pubdate                                           abstract\n",
            "0      1     1975                                                   \n",
            "1      2     1975                                                   \n",
            "2      3     1975                                                   \n",
            "3      4     1975                                                   \n",
            "4      5     1975                                                   \n",
            "..   ...      ...                                                ...\n",
            "95    97     1975                                                   \n",
            "96    95     1975  Results are presented of measuring fibrinogen ...\n",
            "97    98     1975                                                   \n",
            "98    99     1975                                                   \n",
            "99   100     1975                                                   \n",
            "\n",
            "[100 rows x 3 columns] \n",
            "\n",
            "pmid         int64\n",
            "pubdate      int64\n",
            "abstract    object\n",
            "dtype: object \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEQAF4gUV_1X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}